Emily Schultz (ess2183)
COMS 4705 - Natural Language Processing
September 27, 2013


Homework 1: Programming Problems (Q4-6)

*** I'm using my 5 "free" late days on this problem set. ***

This code was written and runs on a computer using python 2.6.6. I've also tested it on the CLIC machines.


REPORT OF RESULTS

Question 4:


Part1: how to run your code step by step


1. Run in terminal:
	python rare.py ner_train.dat
to replace infrequent words (count(x) < 5) in the original training data file (ner_train.dat) with a common symbol _RARE_. This will produce the file: rare_replaced.dat, which is identical to the training data file: ner_train.dat, except the _RARE_ keyword replaces words with counts < 5.

2. Next run:
	python count_freqs.py rare_replaced.dat > ner_rare.counts
to get an output of counts, using _RARE_ keyword replacement.

3. Next run:
	python p4.py ner_rare.counts ner_dev.dat > prediction_file
to run the simple named entity tagger on the _RARE_ replaced data.

4. To evaluate the predictions of the entity tagger, run:
	python eval_ne_tagger.py ner_dev.key prediction_file


Part2: performance for your algorithm (including precision, recall, and F-score).


After running count_freqs.py on the rare_replaced.dat file, the _RARE_ counts (with tags) are below:
19243 WORDTAG O _RARE_
1466 WORDTAG I-LOC _RARE_
9 WORDTAG B-LOC _RARE_
5462 WORDTAG I-PER _RARE_
10 WORDTAG B-MISC _RARE_
863 WORDTAG I-MISC _RARE_
3207 WORDTAG I-ORG _RARE_

Output from eval_ne_tagger.py:

Found 48511 NEs. Expected 5931 NEs; Correct: 1602.

	 	precision 	recall 		F1-Score
Total:	 0.033023	0.270106	0.058852
PER:	 0.000000	0.000000	0.000000
Warning: prediction file does not contain any instances of entity type ORG.
ORG:	 1.000000	0.000000	0.000000
LOC:	 0.033040	0.873501	0.063672
MISC:	 0.000000	0.000000	0.000000


Part3: observations and comments about your experimental results. 


My simple named entity tagger got 1602 correct. It found 48511 named entities and expected 5931. Overall this is not very impressive. The MISC tag received 0.0 for precision, recall, and F1-Score, as did PER tag. The LOC tag had the best results for all three. Overall, precision was about .03, recall about .27, and F1-Score overall was 0.05.


Part4: any additional information that is requested in the problem.


The function that computes emission parameters is in the Hmm class of count_freqs.py, called e().




Question 5:


Part1: how to run your code step by step (make sure your code can run on CLIC). 

1. Run:
	python p5.py tri_prob_in ner_rare.counts > tri_prob_out
to test the trigram q() function (in count_freqs.py Hmm class) on the tri_prob_in list of state trigrams.

2. Run:
	python viterbi.py ner_rare.counts ner_dev.dat > v_prediction_file
to run the viterbi algorithm on ner.counts and ner_dev.dat and produce the v_prediction_file, which contains all the viterbi prediction results.

3. Run:
	python eval_ne_tagger.py ner_dev.key v_prediction_file
to run the evaluation script on the Viterbi algorithm output.


Part2: performance for your algorithm (including precision, recall, and F-score). 


Output from eval_ne_tagger.py:

Found 15528 NEs. Expected 5931 NEs; Correct: 536.

	 	precision 	recall 		F1-Score
Total:	 0.034518	0.090373	0.049956
PER:	 0.034781	0.264962	0.061490
ORG:	 0.007092	0.001495	0.002469
LOC:	 0.037781	0.025627	0.030539
Warning: prediction file does not contain any instances of entity type MISC.
MISC:	 1.000000	0.000000	0.000000


Part3: observations and comments about your experimental results. 


My Viterbi algorithm tagger got 536 correct. It found 15528 named entities and expected 5931. Overall this is even less impressive than the simple named entity tagger. The LOC tag had the best results for all three. This tagger had a more even accuracy among the different tags. Overall, precision was about .03 (about the same as Q4), recall about .09 (much worse than Q4), and F1-Score overall was 0.04 (worse than Q4).


Part4: any additional information that is requested in the problem.

The function that computes trigram estimates is in the Hmm class of count_freqs.py, called q().



Question 6:
Part1: how to run your code step by step (make sure your code can run on CLIC). 

1. Run:
	python keywd.py ner_train.dat
to produce the moretags_replace.dat file with capitals and numerals replaced, as well as rare keyword.

2. Run:
	python count_freqs_6.py moretags_replace.dat > ner_moretags.count
to get the counts with the new _NUM_ and _CAP_ keywords introduced. 
(count_freqs_6.py is the original count_freqs.py file from the class site).

3. Run:
	python p6.py ner_moretags.count ner_dev.dat > moretags_prediction_file
to generate predictions.

4. Run:
	python eval_ne_tagger.py ner_dev.key moretags_prediction_file
to evaluate the predictions with the tagger.

Part2: performance for your algorithm (including precision, recall, and F-score). 

Output from eval_ne_tagger.py (for simple named entity tagger):

Found 19456 NEs. Expected 5931 NEs; Correct: 1601.

		precision 	recall 		F1-Score
Total:	 0.082288	0.269938	0.126128
PER:	 0.000000	0.000000	0.000000
Warning: prediction file does not contain any instances of entity type ORG.
ORG:	 1.000000	0.000000	0.000000
LOC:	 0.082394	0.872955	0.150576
MISC:	 0.000000	0.000000	0.000000


Part3: observations and comments about your experimental results. 

My modified _RARE_ tagger with _CAP_ and _NUM_ got 1601 correct. It found 19456 named entities and expected 5931. Overall it was about on par with Q4, but a little better. Overall, precision was about .08(a little better than Q4), recall about .26 (worse than Q4), and F1-Score overall was 0.126 (much better than Q4).


Part4: any additional information that is requested in the problem.

